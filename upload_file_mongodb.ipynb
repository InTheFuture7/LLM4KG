{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad3f8f7-4da4-425d-8e5c-e5caba610f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "param1= \"../database/resources/xxxx.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7920152-5836-4465-b407-2e8c03b7d985",
   "metadata": {},
   "source": [
    "## 从 neo4j 中获得编号 EC_R_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e6e175-46ac-45b8-9bb4-369172949167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "# 连接Neo4j数据库  \n",
    "uri = \"bolt://localhost:7687\"  # Neo4j数据库URI\n",
    "username = \"neo4j\"  # Neo4j数据库用户名\n",
    "password = \"xxx\"  # Neo4j数据库密码\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(username, password))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c989fd-d004-48aa-8173-652478c99943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_count_with_prefix(driver, prefix):\n",
    "    \"\"\" 计算以特定前缀开头的节点数量 \"\"\"\n",
    "    def count_nodes_with_prefix(tx, prefix):\n",
    "        # 执行Cypher查询，计算以特定前缀开头的节点数量  \n",
    "        result = tx.run(\"MATCH (n) WHERE n.id STARTS WITH $prefix RETURN count(n)\", prefix=prefix)  \n",
    "        return result.single()[0]  \n",
    "\n",
    "    # 使用driver执行查询  \n",
    "    with driver.session() as session:  \n",
    "        count = session.read_transaction(count_nodes_with_prefix, prefix)  \n",
    "    return count  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a270a5-f34e-4a55-a2ac-b17cdd632943",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"EC_R\"  # 查询前缀\n",
    "\n",
    "# 调用函数并打印结果\n",
    "node_count = get_node_count_with_prefix(driver, prefix)\n",
    "\n",
    "print(f\"以'{prefix}'开头的实体节点数量为: {node_count}\")\n",
    "\n",
    "# 关闭driver连接\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d4fb8c-9edb-47b4-b9fa-9f9e6ee05e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import gridfs\n",
    "from gridfs import GridFS\n",
    "\n",
    "# 连接到 MongoDB\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client['mydatabase']  # 访问数据库\n",
    "\n",
    "# 创建GridFSBucket实例\n",
    "bucket = gridfs.GridFSBucket(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fa2d22-f711-4cfc-a0e6-c37d8ac0da80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "\n",
    "# 读取本地/项目的环境变量\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# pip install zhipuai 请先在终端进行安装\n",
    "import os\n",
    "from zhipuai import ZhipuAI\n",
    "client = ZhipuAI(api_key=os.environ[\"ZHIPUAI_API_KEY\"])\n",
    "\n",
    "def get_completion_glm_flash(prompt, model=\"glm-4-flash\", temperature_=0.01, max_tokens_=4095):\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"glm-4-flash\",\n",
    "        messages=[\n",
    "#             {\n",
    "#                 \"role\": \"system\",\n",
    "#                 \"content\": \"你是一个乐于解答各种问题的助手，你的任务是为用户提供专业、准确、有见地的建议。\" \n",
    "#             },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"%s\" % (prompt)\n",
    "            }\n",
    "        ],\n",
    "        top_p= 0.7,\n",
    "        temperature= temperature_,\n",
    "        max_tokens=max_tokens_,\n",
    "        tools = [{\"type\":\"web_search\",\"web_search\":{\"search_result\":False}}],\n",
    "        stream=False\n",
    "    )\n",
    "    if len(response.choices) > 0:\n",
    "        return response.choices[0].message.content\n",
    "    return \"generate answer error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7d1210-85f2-444a-a4a0-97abef451bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pptx import Presentation\n",
    "import pandas as pd\n",
    "from pdf2image import convert_from_path\n",
    "from docx import Document\n",
    "import shutil\n",
    "import requests\n",
    "import base64\n",
    "\n",
    "\n",
    "def handle_other(file_path):\n",
    "    pass\n",
    "\n",
    "# 定义处理不同文件类型的函数\n",
    "def handle_pptx(file_path):\n",
    "    '''获取 ppt 文件中所有文本'''\n",
    "    presentation = Presentation(file_path)\n",
    "    text_runs = []\n",
    "    for slide in presentation.slides:\n",
    "        for shape in slide.shapes:\n",
    "            if hasattr(shape, \"text\"):\n",
    "                text_runs.append(shape.text)\n",
    "    return '\\n'.join(text_runs)\n",
    "\n",
    "\n",
    "def handle_docx(file_path):\n",
    "    \"\"\"\n",
    "    输入：docx文件路径\n",
    "    输出：docx文件中所有文本\n",
    "    \"\"\"\n",
    "    # 初始化一个空字符串，用于存储文档中的所有文本\n",
    "    full_text = []\n",
    "    # 加载Word文档\n",
    "    doc = Document(file_path)\n",
    "    # 遍历文档中的每个段落\n",
    "    for para in doc.paragraphs:\n",
    "        # 将每个段落的文本添加到 full_text 列表中\n",
    "        full_text.append(para.text)\n",
    "    # 将列表中的文本合并为一个单一的字符串\n",
    "    full_text_string = '\\n'.join(full_text)\n",
    "    # 打印提取的文本\n",
    "    return full_text_string\n",
    "\n",
    "# 下面是处理 pdf 的代码，我觉得可以考虑使用 pymupdf 来读取 pdf 文件内容\n",
    "# from langchain.document_loaders.pdf import PyMuPDFLoader 解析 PDF\n",
    "# https://datawhalechina.github.io/llm-universe/#/C3/3.%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86\n",
    "\n",
    "def ocr_images(file_path):\n",
    "    # 图片转base64\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        base64_data = base64.b64encode(f.read()).decode()\n",
    "    url = \"http://127.0.0.1:1224/api/ocr\"\n",
    "    data = {\n",
    "        \"base64\": base64_data,\n",
    "        # 可选参数\n",
    "        \"options\": {\n",
    "            # 通用参数\n",
    "            \"tbpu.parser\": \"single_para\",\n",
    "            \"data.format\": \"text\",\n",
    "            # 引擎参数\n",
    "            \"ocr.cls\": False,\n",
    "            \"ocr.language\": \"models/config_chinese.txt\",\n",
    "            \"ocr.maxSideLen\": 1024\n",
    "        }\n",
    "    }\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    data_str = json.dumps(data)\n",
    "    response = requests.post(url, data=data_str, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        res_dict = json.loads(response.text)\n",
    "        #若\"data.format\": \"txet\",返回识别结果,表示去掉位置信息，仅输出识别结果\n",
    "        return res_dict[\"data\"]\n",
    "        #若\"data.format\": \"dict\",返回全部识别结果，为字典，包含位置信息，匹配度等信息，循环输出识别结果\n",
    "        # for i in res_dict[\"data\"]:\n",
    "        #     print(i[\"text\"])\n",
    "\n",
    "def handle_pdf(file_path):\n",
    "    '''将 pdf 转为 图片，然后调用 umi-ocr http 识别模块，提取 pdf 中的文字\n",
    "    输入：pdf文件路径\n",
    "    输出：pdf中所有文本\n",
    "    '''\n",
    "    image_folder_path = '../database/pdf2img/'\n",
    "    # 如果不存在，则创建图片输出文件夹\n",
    "    if not os.path.exists(image_folder_path):\n",
    "        os.makedirs(image_folder_path)\n",
    "    pages = convert_from_path(file_path)\n",
    "    \n",
    "    # 使用 os.path.basename 获取文件名（包含扩展名）\n",
    "    file_name_with_extension = os.path.basename(file_path)\n",
    "    # 使用 os.path.splitext 分割文件名和扩展名\n",
    "    file_name, file_extension = os.path.splitext(file_name_with_extension)\n",
    "    \n",
    "    for i, page in enumerate(pages):\n",
    "        # 保存图片到指定文件夹\n",
    "        img_path = os.path.join(image_folder_path, f\"{file_name}_{i + 1:03d}.png\")\n",
    "        page.save(img_path, 'PNG')\n",
    "\n",
    "    pdf_text = ''\n",
    "    \n",
    "    # 遍历文件夹中的所有文件\n",
    "    for filename in os.listdir(image_folder_path):\n",
    "        # 由于文件夹中都是PNG图片，直接构造完整的文件路径\n",
    "        file_path = os.path.join(image_folder_path, filename)\n",
    "        # 调用函数处理图片\n",
    "        pdf_text = pdf_text + str(ocr_images(file_path))\n",
    "    # 删除的文件夹\n",
    "    shutil.rmtree(image_folder_path)\n",
    "    return pdf_text\n",
    "\n",
    "import re\n",
    "import jieba\n",
    "def process_text(stopwords_file_path:str, dictionary_file_path:str, text:str) -> list:\n",
    "    \"\"\"\n",
    "    针对文本字符串的中文文本预处理：分词、去停用词、添加字典\n",
    "    输入：停用词表路径、自定义字典路径、待处理的文本\n",
    "    输出：已预处理的列表\n",
    "    \"\"\"\n",
    "\n",
    "    # 正则表达式匹配所有非中文的字符\n",
    "    pattern = re.compile(r'[^\\u4e00-\\u9fa5]')\n",
    "    # 替换匹配到的字符为空字符串\n",
    "    text_cn = re.sub(pattern, '', text)\n",
    "    \n",
    "    jieba.load_userdict(dictionary_file_path)  # 加载自定义词典\n",
    "    result = []\n",
    "    seg_list = jieba.cut(text_cn)\n",
    "    for word in seg_list:\n",
    "        result.append(word)\n",
    "    \n",
    "    # 导入停用词\n",
    "    f = open(stopwords_file_path, 'r', encoding='utf-8')\n",
    "    stop_word_list=[]\n",
    "    for line in f.readlines():\n",
    "        stop_word_list.append(line.strip('\\n'))\n",
    "    line_clean = []  # 去除停用词后的分词结果\n",
    "    for word in result:\n",
    "        if word in stop_word_list:\n",
    "            continue\n",
    "        line_clean.append(word)\n",
    "    # 拼接字符\n",
    "    line_clean = \"\".join(line_clean)\n",
    "    return line_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef75789-f9d2-48bf-a1ad-2a9a8636278a",
   "metadata": {},
   "source": [
    "根据文件类型选择合适的函数读取文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff9036b-affb-420d-acec-5e164877c69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "file_path = param1\n",
    "\n",
    "base_name = os.path.basename(file_path)\n",
    "filename_ = os.path.splitext(base_name)[0]\n",
    "\n",
    "# # 使用 'with' 语句确保文件正确关闭\n",
    "# with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#     content = file.read()\n",
    "\n",
    "# # 输出文件内容\n",
    "# print(content)\n",
    "\n",
    "import os\n",
    "\n",
    "def read_file(file_path):\n",
    "    # 获取文件扩展名\n",
    "    _, file_extension = os.path.splitext(file_path)\n",
    "    \n",
    "    # 根据文件扩展名调用相应的处理函数\n",
    "    if file_extension.lower() == '.pdf':\n",
    "        return handle_pdf(file_path)\n",
    "    elif file_extension.lower() == '.docx':\n",
    "        return handle_docx(file_path)\n",
    "    elif file_extension.lower() == '.pptx':\n",
    "        return handle_pptx(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {file_extension}\")\n",
    "\n",
    "content = read_file(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37e8140-84fe-4e0d-8067-47c10ad0d99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_explanation = get_completion_glm_flash(\"请总结文本内容 %s，以纯文本格式输出，字数限在200字以内\" % content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90cf782-620f-4000-a917-bf6a6966757b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589f4a77-3cc7-4daa-9b6e-77c66c090b07",
   "metadata": {},
   "source": [
    "## 生成文件的元数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cd8cff",
   "metadata": {},
   "source": [
    "启动mongodb：`mongod --dbpath xxxxx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95449829-ecc3-48f3-a32b-2bcf82cd65fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    \"node_id\": f\"EC_R_{(node_count + 1):04}\",\n",
    "    \"name_zh\": filename_,\n",
    "    \"explanation\": file_explanation,\n",
    "    'type': \"resource\",\n",
    "    'timeZoneOffset': \"+08:00\" # 默认为 UTC 时区，时区偏移量 +8 为北京时间\n",
    "}\n",
    "\n",
    "# 读取文件内容到内存\n",
    "with open(file_path, \"rb\") as file:\n",
    "    file_data = file.read()\n",
    "\n",
    "file_id = bucket.upload_from_stream(\n",
    "    filename=filename_,\n",
    "    source=file_data,\n",
    "    metadata=metadata\n",
    ")\n",
    "print(\"File uploaded with metadata.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc757c5-7b36-4246-a83f-da52236470ff",
   "metadata": {},
   "source": [
    "## 获得 neo4j 中知识点实体的所有中文名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d57094-b2cf-4dba-80e1-16e9c1193fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "# 连接Neo4j数据库  \n",
    "uri = \"bolt://localhost:7687\"  # Neo4j 数据库 URI\n",
    "username = \"neo4j\"  # Neo4j数据库用户名\n",
    "password = \"xxx\"  # Neo4j数据库密码\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(username, password))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcff9c49-6b88-4668-9ccc-96367c4a3666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_knowledge_entities():  \n",
    "    with driver.session() as session:  \n",
    "        result = session.run(\"MATCH (n) WHERE n.type = 'knowledge' RETURN n.name_zh AS name_zh\")  \n",
    "        knowledge_entities = [record[\"name_zh\"] for record in result]  \n",
    "        return knowledge_entities\n",
    "\n",
    "# 调用函数并打印结果\n",
    "knowledge_names = fetch_knowledge_entities()\n",
    "print(knowledge_names)\n",
    "\n",
    "# 关闭数据库连接  \n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a54bc8-0820-40f3-b496-682fd7c5ea5e",
   "metadata": {},
   "source": [
    "## RRF 算法排序"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0915cb-f348-4813-9c14-75db4a4862e4",
   "metadata": {},
   "source": [
    "bm25 算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6f9a58-2e33-49fa-8f98-c6d705a77bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "corpus = knowledge_names\n",
    "\n",
    "tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
    "# tokenized_corpus\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "# <rank_bm25.BM25Okapi at 0x1047881d0>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84af810a-a991-4492-a0d5-5ed56b7bc8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = content  # 资料文件中的文本内容\n",
    "tokenized_query = query.split(\" \")\n",
    "\n",
    "doc_scores = bm25.get_scores(tokenized_query)\n",
    "doc_scores\n",
    "# array([0.        , 0.93729472, 0.        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fdfe40-9c45-4b32-8ec7-caf57a2c3d50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted_entity_names_bm25 = bm25.get_top_n(tokenized_query, corpus, n=100)\n",
    "# ['It is quite windy in London']\n",
    "sorted_entity_names_bm25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895408e1",
   "metadata": {},
   "source": [
    "启动 docker 中的 milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6835a9ae-fbab-44ef-ab48-0a3533d579a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)  # 只显示 INFO 级别及以上的日志\n",
    "\n",
    "from pymilvus import connections, utility, FieldSchema, CollectionSchema, DataType, Collection\n",
    "from text2vec import SentenceModel\n",
    "import json\n",
    "\n",
    "\n",
    "def connect_to_milvus():\n",
    "    try:\n",
    "        connections.connect(\"default\", host=\"localhost\", port=\"19530\")\n",
    "        print(\"Connected to Milvus.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to connect to Milvus: {e}\")\n",
    "        raise\n",
    "        \n",
    "def text2vec_embedding(text):\n",
    "    model = SentenceModel('shibing624/text2vec-base-chinese')\n",
    "    embedding = model.encode(text)\n",
    "    return embedding.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0053d87e-d18a-4b87-86aa-10491d4e9025",
   "metadata": {},
   "outputs": [],
   "source": [
    "connect_to_milvus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055e2e8e-1ea6-4059-9851-16c748d3746e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get collection\n",
    "collection_name = \"entity_vec\" \n",
    "collection = Collection(name=collection_name)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba437bf8-e211-4eb9-8b03-72ad090d297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entity_names(results):  \n",
    "    entity_names = []\n",
    "\n",
    "    # 遍历搜索结果，提取 entity_name 和 distance  \n",
    "    for hits in results:  \n",
    "        for hit in hits:  \n",
    "            entity_name = hit.entity.get('entity_name')  \n",
    "            distance = hit.distance  # 假设 distance 是直接在 hit 中  \n",
    "            entity_names.append((entity_name, distance))  \n",
    "\n",
    "    # 按照 distance 升序排序\n",
    "    entity_names.sort(key=lambda x: x[1])  \n",
    "\n",
    "    # 只提取 entity_name 组成的新列表  \n",
    "    sorted_entity_names = [name for name, _ in entity_names]  \n",
    "    return sorted_entity_names  \n",
    "\n",
    "def search_and_query(collection, search_vectors, search_field, search_params):\n",
    "    collection.load()  \n",
    "    # Vector search  \n",
    "    # 将输出字段修改为实际存在的字段，比如 'entity_name'  \n",
    "    result = collection.search(search_vectors, search_field, search_params, limit=100, output_fields=[\"entity_name\"])  \n",
    "    # print_search_results(result, \"Vector search results:\")\n",
    "    sorted_entity_names = extract_entity_names(result)\n",
    "    return sorted_entity_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05b890d300099fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = content  \n",
    "query_vector = text2vec_embedding(query)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb12ba00-9741-45cb-86c4-bf19132fb7b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted_entity_names_milvus = search_and_query(collection, [query_vector], \"embeddings\", {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}})\n",
    "sorted_entity_names_milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08972432-1634-4b9a-ad70-5e9f51791555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rrf_rank_fusion(list1, list2, k=60):  \n",
    "    \"\"\"  \n",
    "    使用RRF算法融合两个排序结果  \n",
    "    \n",
    "    :param list1: 第一个排序结果列表，按优先级从高到低排列  \n",
    "    :param list2: 第二个排序结果列表，按优先级从高到低排列  \n",
    "    :param k: 参数，通常设定为60  \n",
    "    :return: 融合后的排序结果列表  \n",
    "    \"\"\"  \n",
    "\n",
    "    # 创建一个字典来存储每个文档的得分  \n",
    "    score_map = {}  \n",
    "\n",
    "    # 对于第一个列表  \n",
    "    for rank, doc in enumerate(list1):  \n",
    "        score_map[doc] = score_map.get(doc, 0) + (1 / (rank + 1))  \n",
    "\n",
    "    # 对于第二个列表  \n",
    "    for rank, doc in enumerate(list2):  \n",
    "        score_map[doc] = score_map.get(doc, 0) + (1 / (rank + 1 + k))  # 考虑k参数  \n",
    "\n",
    "    # 根据得分进行排序  \n",
    "    merged_list = sorted(score_map.items(), key=lambda item: item[1], reverse=True)  \n",
    "\n",
    "    # 返回排序结果的文档列表  \n",
    "    return [doc for doc, score in merged_list]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8389814cb3c5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = sorted_entity_names_milvus\n",
    "list2 = sorted_entity_names_bm25\n",
    "\n",
    "merged_result = rrf_rank_fusion(list1, list2)  \n",
    "print(\"合并后的排序结果:\", merged_result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff1ccfe-9040-4130-82df-ac9a235f184f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_metadata(filename_):\n",
    "    metadata_neo4j = {}\n",
    "    grid_out_cursor  = bucket.find({'filename': filename_})\n",
    "    for file_metadata in grid_out_cursor:\n",
    "        # file_metadata.filename\n",
    "        metadata_neo4j['node_id'] = file_metadata.metadata['node_id']\n",
    "        metadata_neo4j['name_zh'] = file_metadata.metadata['name_zh']\n",
    "        metadata_neo4j['explanation'] = file_metadata.metadata['explanation']\n",
    "        metadata_neo4j['type'] = file_metadata.metadata['type']\n",
    "        metadata_neo4j['file_id'] = file_metadata._id\n",
    "    return metadata_neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cbc375-f47a-4191-8a7d-1a73a731617d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_neo4j = get_file_metadata(filename_)\n",
    "metadata_neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fc027c-6f9e-48a3-9c84-22cf95978e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_ids(driver, keywords):  \n",
    "    with driver.session() as session:  \n",
    "        result = session.run(  \n",
    "            \"MATCH (n) WHERE n.name_zh IN $keywords RETURN n.id AS node_id\",  \n",
    "            keywords=keywords  \n",
    "        )  \n",
    "        return [record[\"node_id\"] for record in result]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb230e9f5aff3d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查询并打印 node_id  \n",
    "node_ids = get_node_ids(driver, merged_result[0:20])  \n",
    "print(node_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb37a4ff-bd6b-48fa-8366-607a1068ee01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json  \n",
    "\n",
    "def relate_res_know(node_resource_id, node_ids):  \n",
    "    \"\"\"\n",
    "    输入：资料实体的id、相关的知识点实体的id\n",
    "    输出：list类型（可写入json文件）的[{\"A_id\":\"资料实体id\", \"B_id\":\"知识点实体id\", \"type\":\"relate\"}, {}]\n",
    "    \"\"\"\n",
    "    # 构建包含输入数据的字典列表  \n",
    "    data = []  \n",
    "    for node_id in node_ids:  \n",
    "        item = {  \n",
    "            \"A_id\": node_resource_id,  \n",
    "            \"B_id\": node_id,  \n",
    "            \"type\": \"relate\"  \n",
    "        }\n",
    "        data.append(item)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48a5a54-8784-46db-9a40-0e7f2b19fe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_resource_id = metadata_neo4j['node_id']\n",
    "node_ids = node_ids\n",
    "\n",
    "data_relate = relate_res_know(node_resource_id, node_ids)\n",
    "data_relate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca61570d-4297-442a-9a96-1ceed9448b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 包含节点和关系的两类三元组，list类型\n",
    "res_data = [metadata_neo4j] + data_relate \n",
    "res_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d458ec3-fb65-4a18-9408-b6ef4b8173d8",
   "metadata": {},
   "source": [
    "## 在 neo4j 中添加资料实体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343956c9-0308-4447-8b6c-59e347ee6e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(data):\n",
    "    \"\"\"\n",
    "    输入：list类型的实体三元组、关系三元组\n",
    "    \"\"\"\n",
    "    # 使用Neo4j的Python驱动程序创建会话并写入数据\n",
    "    with driver.session() as session:\n",
    "        for item in data:\n",
    "            if item['type'] not in ('include', 'relate', 'order'):\n",
    "                # 执行创建节点的操作\n",
    "                try:\n",
    "                    id = item['node_id']\n",
    "                except KeyError:\n",
    "                    print(item)\n",
    "                type = item['type']\n",
    "                try:\n",
    "                    name_zh = item['name_zh']\n",
    "                except KeyError:\n",
    "                    print(item)\n",
    "                name_en = item.get('name_en', \"\")  # 使用get方法避免name_en不存在时引发错误\n",
    "                try:\n",
    "                    explanation = item['explanation']\n",
    "                except KeyError:\n",
    "                    print(item)\n",
    "                # updatetime = item['updatetime']\n",
    "                # 构建Cypher语句创建节点，并设置属性\n",
    "                session.run(\"CREATE (n:Item {id: $id, type: $type, name_zh: $name_zh, name_en: $name_en, explanation: $explanation})\",\n",
    "                                id=id, type=type, name_zh=name_zh, name_en=name_en, explanation=explanation)\n",
    "                # session.run(\"CREATE (n:Item {id: $id, type: $type, name: $name, name_en: $name_en, explanation: $explanation, updatetime: $updatetime})\",\n",
    "                       # id=id, type=type, name=name, name_en=name_en, explanation=explanation, updatetime=updatetime)\n",
    "    \n",
    "            else:\n",
    "                result_a = session.run(\"MATCH (n) WHERE n.id = $A_id RETURN n\", A_id=item['A_id'])\n",
    "                result_b = session.run(\"MATCH (n) WHERE n.id = $B_id RETURN n\", B_id=item['B_id'])\n",
    "                # 检查找到的节点是否存在\n",
    "                if result_a.single() and result_b.single():\n",
    "                    # 如果两个节点都存在，创建关系\n",
    "                    relationship_type = item['type']\n",
    "                    A_id = item['A_id']\n",
    "                    B_id = item['B_id']\n",
    "                    # 构建Cypher语句，并将关系类型作为字符串的一部分\n",
    "                    cypher_query = (\n",
    "                        f\"MATCH (a), (b) WHERE a.id = $A_id AND b.id = $B_id \"\n",
    "                        f\"CREATE (a)-[:{relationship_type}]->(b)\"\n",
    "                    )\n",
    "                    session.run(cypher_query, A_id=A_id, B_id=B_id)\n",
    "                else:\n",
    "                    print(item['A_id'] + item['B_id'] + item['type'] + \"One or both nodes do not exist.\")\n",
    "                    print(result_a.single())\n",
    "                    print(result_b.single())\n",
    "    \n",
    "    # 关闭Neo4j驱动程序\n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23208f0-d784-42cd-8183-48911347d2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_graph(res_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-universe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
